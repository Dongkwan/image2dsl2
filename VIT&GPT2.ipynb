{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIJw1Sco9Dlv",
        "outputId": "f65340f7-6ac9-451b-c4c5-ddfd6673173e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STJ03tGGpq-0"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "26ay_pYIqE2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from model import GPT2DecoderWithImageFeatures\n",
        "from torchvision import transforms\n",
        "\n",
        "vocabulary = ', { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single <UNK> <PAD>'.split()\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "class Pix2CodeDataset(Dataset):\n",
        "    def __init__(self, data_path, img_transform, dsl_transform, mode=\"train\", split_ratio=0.8):\n",
        "        self.data_path = data_path\n",
        "        self.img_transform = img_transform\n",
        "        self.dsl_transform = dsl_transform\n",
        "        self.mode = mode\n",
        "        self.split_ratio = split_ratio\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        data = []\n",
        "        for root, _, files in os.walk(self.data_path):\n",
        "            for file in files:\n",
        "                if file.endswith(\".png\"):\n",
        "                    img_path = os.path.join(root, file)\n",
        "                    dsl_path = os.path.splitext(img_path)[0] + \".gui\"\n",
        "                    data.append((img_path, dsl_path))\n",
        "\n",
        "        split_index = int(len(data) * self.split_ratio)\n",
        "        if self.mode == \"train\":\n",
        "            return data[:split_index]\n",
        "        elif self.mode == \"val\":\n",
        "            return data[split_index:]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode. Use 'train' or 'val'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, dsl_path = self.data[idx]\n",
        "        img_rgb = Image.open(img_path)\n",
        "        img_grey = img_rgb.convert(\"L\")\n",
        "        img_adapted = img_grey.point(lambda x: 255 if x > 128 else 0)\n",
        "        img_stacked = np.stack((img_adapted, img_adapted, img_adapted), axis=-1)\n",
        "        img_stacked_pil = Image.fromarray(np.uint8(img_stacked), mode='RGB')\n",
        "\n",
        "        with open(dsl_path, \"r\") as f:\n",
        "            dsl_code = f.read()\n",
        "\n",
        "        img_tensor = self.img_transform(img_stacked_pil)\n",
        "        dsl_tokens = self.dsl_transform('<START>\\n' + dsl_code + '\\n<END>')\n",
        "        dsl_tensor = torch.LongTensor(dsl_tokens)\n",
        "\n",
        "        return img_tensor, dsl_tensor\n",
        "\n",
        "# Initialize ViT model, dataset, and data loader\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').base_model\n",
        "\n",
        "# Replace the dsl_transform with the tokenizer.encode method\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "dsl_transform = tokenizer.encode\n",
        "\n",
        "# Create train and validation data loaders\n",
        "data_path = \"data/all_data/data\"\n",
        "\n",
        "train_dataset = Pix2CodeDataset(data_path, img_transform, dsl_transform, mode=\"train\")\n",
        "val_dataset = Pix2CodeDataset(data_path, img_transform, dsl_transform, mode=\"val\")\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    imgs, dsls = zip(*batch)\n",
        "\n",
        "    # Pad DSL sequences\n",
        "    max_len = max([len(dsl) for dsl in dsls])\n",
        "    end_token = tokenizer.encode('<PAD>')[0]\n",
        "    padded_dsls = []\n",
        "    for dsl in dsls:\n",
        "        padded_dsls.append(torch.cat([dsl, torch.full((max_len - len(dsl),), end_token, dtype=torch.long        )]))\n",
        "\n",
        "    # Stack padded DSL sequences and images\n",
        "    img_tensor = torch.stack(imgs)\n",
        "    dsl_tensor = torch.stack(padded_dsls)\n",
        "\n",
        "    return img_tensor, dsl_tensor\n",
        "\n",
        "batch_size = 32\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn)\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 768\n",
        "num_layers = 6\n",
        "epochs = 1000\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Initialize the decoder, loss function, and optimizer\n",
        "decoder = GPT2DecoderWithImageFeatures(input_size)\n",
        "decoder.gpt.resize_token_embeddings(len(tokenizer))  # Update the GPT2 model with the new tokenizer\n",
        "decoder.load_state_dict(torch.load(\"best_decoder.pth\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize the learning rate scheduler with warm-up\n",
        "num_warmup_steps = 500\n",
        "num_training_steps = epochs * len(train_data_loader)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "# Initialize variables to track the best validation loss and epoch\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = 0\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "vit_model.eval()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (img_tensor, dsl_tensor) in enumerate(train_data_loader):\n",
        "        loss = 0\n",
        "        decoder.train()\n",
        "        img_tensor = img_tensor.to(device)\n",
        "        dsl_tensor = dsl_tensor.to(device)\n",
        "\n",
        "        # Extract image features using ViT model\n",
        "        with torch.no_grad():\n",
        "            image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Training loop for the GPT2DecoderWithImageFeatures\n",
        "        input_tokens = dsl_tensor[:, :-1]\n",
        "        target_tokens = dsl_tensor[:, 1:]\n",
        "       \n",
        "        output = decoder(input_tokens, image_features)\n",
        "\n",
        "        # print(output.shape)\n",
        "        # print(target_tokens.shape)\n",
        "\n",
        "        loss = criterion(output.permute(0, 2, 1), target_tokens)\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Step {i}/{len(train_data_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (img_tensor, dsl_tensor) in enumerate(val_data_loader):\n",
        "            img_tensor = img_tensor.to(device)\n",
        "            dsl_tensor = dsl_tensor.to(device)\n",
        "\n",
        "            image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "            input_tokens = dsl_tensor[:, :-1]\n",
        "            target_tokens = dsl_tensor[:, 1:]     \n",
        "\n",
        "            output = decoder(input_tokens, image_features)\n",
        "\n",
        "            val_loss += criterion(output.permute(0, 2, 1), target_tokens).item()\n",
        "\n",
        "    val_loss /= len(val_data_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}\")\n",
        "    # Save the best model weights\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(decoder.state_dict(), \"best_decoder.pth\")\n",
        "\n",
        "print(f\"Best model weights saved from epoch {best_epoch+1} with validation loss {best_val_loss}\")\n",
        "\n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "dVroT8STqSck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "from PIL import Image\n",
        "from model import GPT2DecoderWithImageFeatures\n",
        "from transformers import ViTModel\n",
        "\n",
        "vocabulary = ', { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single <UNK> <PAD>'.split()\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "\n",
        "def generate_code(image_path, tokenizer, vit_model, decoder, max_length=512):\n",
        "    # Load and preprocess the image\n",
        "    img_rgb = Image.open(image_path)\n",
        "    img_grey = img_rgb.convert(\"L\")\n",
        "    img_adapted = img_grey.point(lambda x: 255 if x > 128 else 0)\n",
        "    img_stacked = np.stack((img_adapted, img_adapted, img_adapted), axis=-1)\n",
        "    img_stacked_pil = Image.fromarray(np.uint8(img_stacked), mode='RGB')\n",
        "    img_tensor = img_transform(img_stacked_pil)\n",
        "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Extract image features using ViT model\n",
        "    with torch.no_grad():\n",
        "        image_features = vit_model(img_tensor).last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Prepare the initial input for the decoder\n",
        "    input_ids = tokenizer.encode('<START>', return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate code using the decoder\n",
        "    decoder.eval()\n",
        "    generated_code = []\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = decoder(input_ids, image_features)\n",
        "        \n",
        "        next_token_id = torch.argmax(output, dim=-1)[:, -1]\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "        \n",
        "        if next_token_id.item() == tokenizer.encode('<END>')[0]:\n",
        "            break\n",
        "\n",
        "        generated_code.append(tokenizer.decode(next_token_id))\n",
        "\n",
        "    return ''.join(generated_code)\n",
        "\n",
        "# Load the tokenizer, ViT model, and decoder\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "special_tokens_dict = {'additional_special_tokens': vocabulary}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').base_model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model.to(device)\n",
        "\n",
        "decoder = GPT2DecoderWithImageFeatures(input_size=768)\n",
        "decoder.gpt.resize_token_embeddings(len(tokenizer))  # Update the GPT2 model with the new tokenizer\n",
        "\n",
        "decoder.load_state_dict(torch.load(\"best_decoder.pth\"))\n",
        "decoder.to(device)\n",
        "\n",
        "# Perform inference on a test image\n",
        "image_path = \"images.png\"\n",
        "generated_code = generate_code(image_path, tokenizer, vit_model, decoder)\n",
        "print(\"Generated code:\\n\", generated_code)\n"
      ],
      "metadata": {
        "id": "9gte0ZiUuhT3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}